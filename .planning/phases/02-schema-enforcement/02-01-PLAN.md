---
phase: 02-schema-enforcement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/backend/runners/github/services/pydantic_models.py
  - tests/test_structured_outputs.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Findings without verification evidence fail Pydantic validation"
    - "Empty code_examined is rejected (min_length=1)"
    - "Invalid verification_method is rejected (Literal constraint)"
    - "Line range must be exactly 2 integers"
    - "is_impact_finding field exists and defaults to False"
    - "checked_for_handling_elsewhere field exists and defaults to False"
  artifacts:
    - path: "apps/backend/runners/github/services/pydantic_models.py"
      provides: "VerificationEvidence class, updated BaseFinding, updated ParallelOrchestratorFinding"
      contains: "class VerificationEvidence"
    - path: "tests/test_structured_outputs.py"
      provides: "Tests for required verification field"
      contains: "class TestVerificationEvidence"
  key_links:
    - from: "BaseFinding"
      to: "VerificationEvidence"
      via: "required verification field"
      pattern: "verification: VerificationEvidence"
    - from: "ParallelOrchestratorFinding"
      to: "VerificationEvidence"
      via: "required verification field"
      pattern: "verification: VerificationEvidence"
---

<objective>
Add VerificationEvidence class and make evidence-based verification required for all findings.

Purpose: Shift quality control from programmatic filters to schema enforcement. When a finding exists, it MUST have proof. This is the foundation for removing downstream filters (Phase 5).

Output: Modified pydantic_models.py with required verification fields, tests proving schema enforcement works.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-schema-enforcement/02-RESEARCH.md
@apps/backend/runners/github/services/pydantic_models.py
@tests/test_structured_outputs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add VerificationEvidence and update finding classes</name>
  <files>apps/backend/runners/github/services/pydantic_models.py</files>
  <action>
Create VerificationEvidence class following FindingValidationResult pattern (lines 589-637):

```python
class VerificationEvidence(BaseModel):
    """Evidence that a finding was verified against actual code."""

    code_examined: str = Field(
        min_length=1,
        description=(
            "REQUIRED: Exact code snippet that was examined. "
            "Must be actual code from the file, not a description of code. "
            "Copy-paste the relevant lines directly."
        ),
    )
    line_range_examined: list[int] = Field(
        min_length=2,
        max_length=2,
        description=(
            "Start and end line numbers [start, end] of the examined code. "
            "Must match the code in code_examined."
        ),
    )
    verification_method: Literal[
        "direct_code_inspection",
        "cross_file_trace",
        "test_verification",
        "dependency_analysis"
    ] = Field(
        description=(
            "How the issue was verified: "
            "direct_code_inspection = found issue directly in the code shown; "
            "cross_file_trace = traced through imports/calls to find the issue; "
            "test_verification = verified through examination of test code; "
            "dependency_analysis = verified through analyzing dependencies"
        )
    )
```

Place this class BEFORE BaseFinding (around line 35).

Update BaseFinding (lines 36-53):
1. Mark existing `evidence` field as DEPRECATED in its description
2. Add required `verification: VerificationEvidence` field (no default value)

Update ParallelOrchestratorFinding (lines 377-412):
1. Mark existing `evidence` field as DEPRECATED in its description
2. Add required `verification: VerificationEvidence` field (no default value)
3. Add `is_impact_finding: bool = Field(False, description="...")` - for findings about OTHER files
4. Add `checked_for_handling_elsewhere: bool = Field(False, description="...")` - for "missing X" claims

DO NOT modify other finding classes (SecurityFinding, QualityFinding, etc.) - they inherit from BaseFinding.
DO NOT remove the old `evidence` field yet - mark as deprecated for transition.
  </action>
  <verify>
Run: `cd /Users/andremikalsen/Documents/Coding/autonomous-coding && python -c "from apps.backend.runners.github.services.pydantic_models import VerificationEvidence, BaseFinding, ParallelOrchestratorFinding; print('Import OK')"` - should succeed.

Verify schema includes new fields:
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding && python -c "
from apps.backend.runners.github.services.pydantic_models import ParallelOrchestratorFinding
schema = ParallelOrchestratorFinding.model_json_schema()
assert 'verification' in schema['properties'], 'Missing verification field'
assert 'is_impact_finding' in schema['properties'], 'Missing is_impact_finding field'
assert 'checked_for_handling_elsewhere' in schema['properties'], 'Missing checked_for_handling_elsewhere field'
print('Schema verification OK')
"
```
  </verify>
  <done>
VerificationEvidence class exists with code_examined, line_range_examined, verification_method fields.
BaseFinding has required verification field.
ParallelOrchestratorFinding has required verification, is_impact_finding, checked_for_handling_elsewhere fields.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for schema enforcement</name>
  <files>tests/test_structured_outputs.py</files>
  <action>
Add imports at top (after existing imports around line 42):
```python
from pydantic_models import (
    # ... existing imports ...
    VerificationEvidence,
    ParallelOrchestratorFinding,
    BaseFinding,
)
```

Add test classes at end of file:

```python
class TestVerificationEvidence:
    """Tests for VerificationEvidence model."""

    def test_valid_verification(self):
        """Test valid verification evidence."""
        data = {
            "code_examined": "def process_input(user_input):\n    return eval(user_input)",
            "line_range_examined": [10, 11],
            "verification_method": "direct_code_inspection",
        }
        result = VerificationEvidence.model_validate(data)
        assert "eval" in result.code_examined
        assert result.line_range_examined == [10, 11]
        assert result.verification_method == "direct_code_inspection"

    def test_empty_code_examined_rejected(self):
        """Test that empty code_examined is rejected."""
        data = {
            "code_examined": "",
            "line_range_examined": [1, 5],
            "verification_method": "direct_code_inspection",
        }
        with pytest.raises(ValidationError) as exc_info:
            VerificationEvidence.model_validate(data)
        assert "code_examined" in str(exc_info.value)

    def test_invalid_line_range_rejected(self):
        """Test that invalid line ranges are rejected."""
        data = {
            "code_examined": "some code",
            "line_range_examined": [1],  # Should have exactly 2 elements
            "verification_method": "direct_code_inspection",
        }
        with pytest.raises(ValidationError) as exc_info:
            VerificationEvidence.model_validate(data)
        assert "line_range_examined" in str(exc_info.value)

    def test_invalid_verification_method_rejected(self):
        """Test that invalid verification method is rejected."""
        data = {
            "code_examined": "some code",
            "line_range_examined": [1, 5],
            "verification_method": "guessed",  # Invalid method
        }
        with pytest.raises(ValidationError) as exc_info:
            VerificationEvidence.model_validate(data)
        assert "verification_method" in str(exc_info.value)

    def test_all_verification_methods(self):
        """Test all valid verification methods."""
        methods = [
            "direct_code_inspection",
            "cross_file_trace",
            "test_verification",
            "dependency_analysis"
        ]
        for method in methods:
            data = {
                "code_examined": "code",
                "line_range_examined": [1, 5],
                "verification_method": method,
            }
            result = VerificationEvidence.model_validate(data)
            assert result.verification_method == method


class TestParallelOrchestratorFindingVerification:
    """Tests for verification field requirement on ParallelOrchestratorFinding."""

    def test_missing_verification_rejected(self):
        """Test that findings without verification are rejected."""
        data = {
            "id": "test-1",
            "file": "test.py",
            "line": 10,
            "title": "Test finding",
            "description": "A test finding without verification",
            "category": "quality",
            "severity": "medium",
            # No verification field - should fail
        }
        with pytest.raises(ValidationError) as exc_info:
            ParallelOrchestratorFinding.model_validate(data)
        assert "verification" in str(exc_info.value)

    def test_valid_finding_with_verification(self):
        """Test valid finding with verification evidence."""
        data = {
            "id": "test-1",
            "file": "test.py",
            "line": 10,
            "title": "SQL Injection vulnerability",
            "description": "User input passed directly to query",
            "category": "security",
            "severity": "critical",
            "verification": {
                "code_examined": "cursor.execute(f'SELECT * FROM users WHERE id={user_id}')",
                "line_range_examined": [10, 10],
                "verification_method": "direct_code_inspection",
            },
        }
        result = ParallelOrchestratorFinding.model_validate(data)
        assert result.verification.code_examined is not None
        assert result.verification.verification_method == "direct_code_inspection"

    def test_is_impact_finding_default_false(self):
        """Test is_impact_finding defaults to False."""
        data = {
            "id": "test-1",
            "file": "test.py",
            "line": 10,
            "title": "Test",
            "description": "Test",
            "category": "quality",
            "severity": "medium",
            "verification": {
                "code_examined": "code",
                "line_range_examined": [10, 10],
                "verification_method": "direct_code_inspection",
            },
        }
        result = ParallelOrchestratorFinding.model_validate(data)
        assert result.is_impact_finding is False

    def test_is_impact_finding_true(self):
        """Test is_impact_finding can be set True."""
        data = {
            "id": "test-1",
            "file": "caller.py",
            "line": 50,
            "title": "Breaking change affects caller",
            "description": "This file calls the changed function and will break",
            "category": "logic",
            "severity": "high",
            "is_impact_finding": True,
            "verification": {
                "code_examined": "result = changed_function(x)",
                "line_range_examined": [50, 50],
                "verification_method": "cross_file_trace",
            },
        }
        result = ParallelOrchestratorFinding.model_validate(data)
        assert result.is_impact_finding is True

    def test_checked_for_handling_elsewhere_default_false(self):
        """Test checked_for_handling_elsewhere defaults to False."""
        data = {
            "id": "test-1",
            "file": "test.py",
            "line": 10,
            "title": "Missing error handling",
            "description": "No try-catch",
            "category": "quality",
            "severity": "medium",
            "verification": {
                "code_examined": "code",
                "line_range_examined": [10, 10],
                "verification_method": "direct_code_inspection",
            },
        }
        result = ParallelOrchestratorFinding.model_validate(data)
        assert result.checked_for_handling_elsewhere is False

    def test_checked_for_handling_elsewhere_true(self):
        """Test checked_for_handling_elsewhere can be set True."""
        data = {
            "id": "test-1",
            "file": "api.py",
            "line": 25,
            "title": "Missing error handling",
            "description": "No try-catch around database call",
            "category": "quality",
            "severity": "medium",
            "checked_for_handling_elsewhere": True,
            "verification": {
                "code_examined": "result = db.query(user_input)",
                "line_range_examined": [25, 25],
                "verification_method": "cross_file_trace",
            },
        }
        result = ParallelOrchestratorFinding.model_validate(data)
        assert result.checked_for_handling_elsewhere is True


class TestVerificationSchemaGeneration:
    """Tests for JSON schema generation with VerificationEvidence."""

    def test_verification_in_parallel_orchestrator_schema(self):
        """Test that VerificationEvidence appears in schema."""
        schema = ParallelOrchestratorFinding.model_json_schema()

        # verification should be in properties
        assert "verification" in schema["properties"]

        # Check $defs includes VerificationEvidence
        assert "$defs" in schema
        assert "VerificationEvidence" in schema["$defs"]

        # Check VerificationEvidence has correct fields
        ve_schema = schema["$defs"]["VerificationEvidence"]
        assert "code_examined" in ve_schema["properties"]
        assert "line_range_examined" in ve_schema["properties"]
        assert "verification_method" in ve_schema["properties"]

    def test_new_boolean_fields_in_schema(self):
        """Test is_impact_finding and checked_for_handling_elsewhere in schema."""
        schema = ParallelOrchestratorFinding.model_json_schema()

        assert "is_impact_finding" in schema["properties"]
        assert "checked_for_handling_elsewhere" in schema["properties"]
```
  </action>
  <verify>
Run tests: `cd /Users/andremikalsen/Documents/Coding/autonomous-coding && apps/backend/.venv/bin/pytest tests/test_structured_outputs.py -v`

All tests should pass, including:
- test_valid_verification
- test_empty_code_examined_rejected
- test_invalid_line_range_rejected
- test_invalid_verification_method_rejected
- test_missing_verification_rejected
- test_valid_finding_with_verification
- test_is_impact_finding_default_false
- test_checked_for_handling_elsewhere_default_false
- test_verification_in_parallel_orchestrator_schema
  </verify>
  <done>
All new tests pass. Schema enforcement is verified:
- Empty code_examined rejected
- Invalid verification_method rejected
- Missing verification on findings rejected
- is_impact_finding and checked_for_handling_elsewhere fields work correctly
- JSON schema includes VerificationEvidence in $defs
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Schema import works:**
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding && python -c "
from apps.backend.runners.github.services.pydantic_models import (
    VerificationEvidence, BaseFinding, ParallelOrchestratorFinding
)
print('Imports OK')
"
```

2. **All tests pass:**
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding && apps/backend/.venv/bin/pytest tests/test_structured_outputs.py -v
```

3. **Schema enforcement verified manually:**
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding && python -c "
from pydantic import ValidationError
from apps.backend.runners.github.services.pydantic_models import ParallelOrchestratorFinding

# This should fail - no verification
try:
    ParallelOrchestratorFinding.model_validate({
        'id': 'test', 'file': 'test.py', 'line': 1,
        'title': 'Test', 'description': 'Test',
        'category': 'quality', 'severity': 'low'
    })
    print('ERROR: Should have failed without verification')
except ValidationError:
    print('PASS: Missing verification correctly rejected')
"
```
</verification>

<success_criteria>
- SCHEMA-01: VerificationEvidence class exists with code_examined (min_length=1), line_range_examined (length 2), verification_method (Literal)
- SCHEMA-02: BaseFinding.verification is required (no default)
- SCHEMA-03: ParallelOrchestratorFinding.verification is required (no default)
- SCHEMA-04: is_impact_finding field exists on ParallelOrchestratorFinding with default False
- SCHEMA-05: checked_for_handling_elsewhere field exists on ParallelOrchestratorFinding with default False
- All existing tests still pass
- New tests verify schema enforcement works
</success_criteria>

<output>
After completion, create `.planning/phases/02-schema-enforcement/02-01-SUMMARY.md`
</output>
