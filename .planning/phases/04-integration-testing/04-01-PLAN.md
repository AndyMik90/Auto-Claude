---
phase: 04-integration-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_integration_phase4.py
autonomous: true

must_haves:
  truths:
    - "False positive rate can be measured from validation_status counts"
    - "Confidence tier routing correctly classifies HIGH/MEDIUM/LOW"
    - "Multi-agent cross-validation boosts confidence when agents agree"
    - "Evidence validation filters findings without code syntax"
    - "Scope filtering rejects findings outside PR changed files"
    - "Path alias imports are detected and resolved"
    - "Python relative imports are detected via AST"
    - "Reverse dependencies are found for changed files"
  artifacts:
    - path: "tests/test_integration_phase4.py"
      provides: "Integration tests for Phase 1-3 features"
      min_lines: 300
      exports: ["TestConfidenceTierRouting", "TestCrossValidation", "TestEvidenceValidation", "TestImportDetection", "TestReverseDepDetection"]
  key_links:
    - from: "tests/test_integration_phase4.py"
      to: "apps/backend/runners/github/services/parallel_orchestrator_reviewer.py"
      via: "import ConfidenceTier, _validate_finding_evidence"
      pattern: "from.*parallel_orchestrator_reviewer import"
    - from: "tests/test_integration_phase4.py"
      to: "apps/backend/runners/github/context_gatherer.py"
      via: "import PRContextGatherer"
      pattern: "from context_gatherer import"
---

<objective>
Create comprehensive integration tests validating all Phase 1-3 features work correctly.

Purpose: Verify the PR review system improvements are functioning as designed, measure false positive reduction, and ensure latency is acceptable.

Output: A test file `tests/test_integration_phase4.py` covering:
- Confidence tier routing (HIGH >= 0.8, MEDIUM 0.5-0.8, LOW < 0.5)
- Multi-agent cross-validation (+0.15 boost, 0.95 cap)
- Evidence validation (code syntax required)
- Scope filtering (file in changed files, valid line numbers)
- Path alias import detection (@/utils, @shared/*)
- Python import detection (relative and absolute)
- Reverse dependency detection
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-integration-testing/04-RESEARCH.md

# Key implementation files to test
@apps/backend/runners/github/services/parallel_orchestrator_reviewer.py
@apps/backend/runners/github/context_gatherer.py
@apps/backend/runners/github/models.py

# Existing test patterns to follow
@tests/test_github_pr_e2e.py
@tests/test_output_validator.py
@tests/test_context_gatherer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Phase 1 Feature Tests</name>
  <files>tests/test_integration_phase4.py</files>
  <action>
Create `tests/test_integration_phase4.py` with tests for Phase 1 features:

1. **TestConfidenceTierRouting** class:
   - `test_high_confidence_tier()` - Verify >= 0.8 returns HIGH
   - `test_medium_confidence_tier()` - Verify 0.5-0.8 returns MEDIUM
   - `test_low_confidence_tier()` - Verify < 0.5 returns LOW
   - `test_boundary_values()` - Test 0.5 (MEDIUM) and 0.8 (HIGH) exact boundaries
   - Import from `parallel_orchestrator_reviewer`: `ConfidenceTier`

2. **TestEvidenceValidation** class:
   - `test_valid_evidence_with_code_syntax()` - Evidence with =, (), {} passes
   - `test_invalid_evidence_no_code_syntax()` - Prose-only evidence fails
   - `test_empty_evidence_fails()` - Empty/short evidence fails
   - `test_evidence_with_function_def()` - "def " or "function " passes
   - Import from `parallel_orchestrator_reviewer`: `_validate_finding_evidence`
   - Create PRReviewFinding fixtures with various evidence values

3. **TestScopeFiltering** class:
   - `test_finding_in_changed_files_passes()` - file in changed_files passes
   - `test_finding_outside_changed_files_filtered()` - file not in changed_files filtered
   - `test_invalid_line_number_filtered()` - line > file length filtered
   - `test_impact_finding_allowed()` - Finding with "breaks"/"affects" keyword allowed for unchanged files

Follow existing patterns from `test_output_validator.py`:
- Use `@pytest.fixture` for sample findings and changed_files
- Use descriptive docstrings
- Group related tests in classes
- Use `sys.path.insert` pattern for imports

Header:
```python
"""
Integration Tests for PR Review System - Phase 4
=================================================

Tests validating all Phase 1-3 features work correctly:
- Phase 1: Confidence routing, evidence validation, scope filtering
- Phase 2: Import detection (path aliases, Python), reverse dependencies
- Phase 3: Multi-agent cross-validation
"""
```
  </action>
  <verify>
Run: `apps/backend/.venv/bin/pytest tests/test_integration_phase4.py -v -k "Phase1 or Confidence or Evidence or Scope" --tb=short`
Expected: All Phase 1 tests pass
  </verify>
  <done>
- TestConfidenceTierRouting has 4+ tests covering tier boundaries
- TestEvidenceValidation has 4+ tests covering code syntax detection
- TestScopeFiltering has 3+ tests covering scope filtering logic
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Phase 2 and Phase 3 Feature Tests</name>
  <files>tests/test_integration_phase4.py</files>
  <action>
Add to `tests/test_integration_phase4.py` tests for Phase 2 and Phase 3 features:

1. **TestImportDetection** class (Phase 2):
   - `test_path_alias_detection()` - @/utils imports detected
   - `test_commonjs_require_detection()` - require('./utils') detected
   - `test_reexport_detection()` - export * from detected
   - `test_python_relative_import()` - from .utils import detected
   - `test_python_absolute_import()` - from utils import detected
   - Use `PRContextGatherer._find_imports()` directly
   - Create temp project structures with tsconfig.json for alias tests
   - Mock minimal file system using tempfile

2. **TestReverseDepDetection** class (Phase 2):
   - `test_finds_files_importing_changed_file()` - Verify grep-based detection
   - `test_skips_generic_names()` - index, main, utils skipped
   - `test_respects_timeout()` - Large repo doesn't hang (mocked)
   - Use `PRContextGatherer._find_dependents()` or the public method

3. **TestCrossValidation** class (Phase 3):
   - `test_multi_agent_agreement_boosts_confidence()` - 2+ agents on same finding -> +0.15
   - `test_confidence_boost_capped_at_095()` - 0.85 + 0.15 = 0.95 (not 1.0)
   - `test_merged_finding_has_cross_validated_true()` - Flag set correctly
   - `test_grouping_by_file_line_category()` - Same (file, line, category) groups
   - `test_merged_description_combines_sources()` - Descriptions joined with ' | '
   - Use `ParallelOrchestratorReviewer._cross_validate_findings()`
   - Create PRReviewFinding fixtures with same file/line from different agents

For import tests, create temp directories with:
- tsconfig.json with paths: `{"@/*": ["src/*"]}`
- Python files with relative imports

For cross-validation tests:
- Create findings with matching (file, line, category) tuples
- Verify merged result has boosted confidence
  </action>
  <verify>
Run: `apps/backend/.venv/bin/pytest tests/test_integration_phase4.py -v --tb=short`
Expected: All tests pass (Phase 1, 2, and 3 features)
  </verify>
  <done>
- TestImportDetection has 5 tests covering JS/TS and Python imports
- TestReverseDepDetection has 3 tests covering reverse dependency logic
- TestCrossValidation has 5 tests covering multi-agent agreement
- All tests pass when run with pytest
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Metrics Collection and False Positive Rate Tests</name>
  <files>tests/test_integration_phase4.py</files>
  <action>
Add metrics collection tests to `tests/test_integration_phase4.py`:

1. **TestFalsePositiveMetrics** class:
   - `test_calculate_false_positive_rate()` - FP rate = dismissed / total
   - `test_fp_rate_with_no_findings()` - Returns 0.0 not divide-by-zero
   - `test_fp_rate_target_under_5_percent()` - Verify calculation matches target

   Helper function:
   ```python
   def calculate_false_positive_rate(findings: list[PRReviewFinding]) -> float:
       """Calculate FP rate from validation_status field."""
       dismissed = sum(1 for f in findings
                       if f.validation_status == "dismissed_false_positive")
       total = len(findings)
       return dismissed / total if total > 0 else 0.0
   ```

2. **TestMetricsIntegration** class:
   - `test_metrics_from_full_review_result()` - Extract metrics from PRReviewResult
   - `test_severity_distribution_counts()` - Count findings by severity
   - `test_category_distribution_counts()` - Count findings by category
   - `test_validation_status_distribution()` - Count by validation_status

3. **TestFullPipeline** class (integration):
   - `test_scenario_false_positive_caught()` - Scenario where FP would have occurred
     - Create finding with vague evidence
     - Verify _validate_finding_evidence rejects it
     - Verify it would be filtered in pipeline
   - `test_scenario_valid_finding_passes()` - Valid finding goes through
     - Create finding with good evidence in changed file
     - Verify it passes all validations
   - `test_scenario_multi_agent_high_confidence()` - Cross-validation scenario
     - Create 2 findings from different agents on same issue
     - Verify merged finding has HIGH tier after boost

These tests verify the SUCCESS METRICS from ROADMAP.md:
- False positive rate < 5%
- Finding accuracy > 90% (via validation_status tracking)
  </action>
  <verify>
Run: `apps/backend/.venv/bin/pytest tests/test_integration_phase4.py -v --tb=short`
Expected: All tests pass including metrics tests
Count: At least 20 total tests in the file
  </verify>
  <done>
- TestFalsePositiveMetrics has 3 tests for FP rate calculation
- TestMetricsIntegration has 4 tests for metrics extraction
- TestFullPipeline has 3 end-to-end scenario tests
- Total test count is 20+ covering all Phase 1-3 features
- `pytest tests/test_integration_phase4.py -v` shows all tests passing
  </done>
</task>

</tasks>

<verification>
1. Test file exists: `tests/test_integration_phase4.py`
2. All tests pass: `apps/backend/.venv/bin/pytest tests/test_integration_phase4.py -v`
3. Test count >= 20: `apps/backend/.venv/bin/pytest tests/test_integration_phase4.py --collect-only | grep "test session starts" -A 100 | grep "<Function"`
4. Coverage of all features:
   - Phase 1: ConfidenceTier, evidence validation, scope filtering
   - Phase 2: Path alias imports, Python imports, reverse deps
   - Phase 3: Cross-validation, confidence boost
</verification>

<success_criteria>
1. `tests/test_integration_phase4.py` exists with 300+ lines
2. All tests pass when run with pytest
3. Tests cover: confidence routing, evidence validation, scope filtering, import detection, reverse deps, cross-validation
4. False positive rate calculation implemented and tested
5. Existing tests in `tests/` still pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/04-integration-testing/04-01-SUMMARY.md` using the standard summary template.
</output>
