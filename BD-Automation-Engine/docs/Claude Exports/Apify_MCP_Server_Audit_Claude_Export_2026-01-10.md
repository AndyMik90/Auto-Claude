# Apify MCP Server Capabilities Audit - Claude Export
**Date**: January 10, 2026  
**Project**: PTS DCGS Business Development Campaign  
**Focus**: Apify MCP Server Integration & Job Scraping Automation

---

## 1. CONVERSATION SUMMARY

### Topic/Focus Area
**Apify MCP Server Capabilities Audit & Integration Strategy**

### Date Range
January 10, 2026 (Single session)

### Primary Objective
Conduct a comprehensive audit of the Apify MCP (Model Context Protocol) Server capabilities available through the custom MCP connection in Claude, with specific focus on:

1. **Catalog all available MCP tools** - Document every tool, input parameter, and use case
2. **Identify job scraping Actors** - Find optimal Actors for LinkedIn, Indeed, and competitor portals
3. **Analyze current implementation** - Review existing Puppeteer scraper setup and data pipeline
4. **Recommend improvements** - Propose integration strategy for DCGS BD campaign automation
5. **Calculate ROI** - Quantify cost savings and efficiency gains from automation
6. **Create implementation roadmap** - Provide week-by-week action plan

### Context
PTS (Prime Technical Services) operates a business development campaign targeting GDIT's $950M DCGS program portfolio. The current workflow involves:
- Scraping competitor job boards using `apify/puppeteer-scraper`
- Manual CSV import to Notion Program Mapping Intelligence Hub
- GPT-4o enrichment via copy/paste to ChatGPT
- Contact discovery through ZoomInfo/LinkedIn
- BD playbook and call sheet generation

The audit aimed to identify automation opportunities using the Apify MCP Server to reduce manual work and improve data quality.

---

## 2. TECHNICAL DECISIONS MADE

### Decision 1: Document MCP Tools Comprehensively
**Decision**: Create full audit document covering all 20+ MCP tools with detailed parameters, use cases, and examples

**Reasoning**: 
- User needs authoritative reference for all available capabilities
- Future implementation requires understanding tool limitations and best practices
- Documentation enables training of BD team on new workflows

**Alternatives Considered**: 
- Quick summary of top 5 tools only
- Rejected because comprehensive reference provides more long-term value

---

### Decision 2: Recommend LinkedIn Scraping as Priority #1
**Decision**: Identified `curious_coder/linkedin-jobs-scraper` as immediate implementation target

**Reasoning**:
- LinkedIn provides hiring manager names and contact info (vs anonymous job boards)
- 98.7% success rate with 11,674+ users validates reliability
- $1 per 1,000 results extremely cost-effective vs manual labor
- GDIT program managers predominantly use LinkedIn for recruiting

**Alternatives Considered**:
- `fantastic-jobs/advanced-linkedin-job-search-api` - More expensive ($0.005/job) but includes AI enrichments
- `worldunboxer/rapid-linkedin-scraper` - Free but lower rating (4.36 vs 4.85)
- **Selected curious_coder for balance of cost, reliability, and features**

---

### Decision 3: Enable Storage Tools Category
**Decision**: Recommend enabling all storage-related MCP tools (currently disabled)

**Reasoning**:
- Historical trend analysis requires access to past scraping datasets
- Week-over-week job volume changes signal contract option exercises or recompetes
- Competitive intelligence needs comparison across time periods
- Tools like `get-dataset-items` provide advanced filtering unavailable in `get-actor-output`

**Alternatives Considered**:
- Keep storage tools disabled and export CSVs manually
- Rejected due to automation bottleneck and lack of query flexibility

---

### Decision 4: Maintain Puppeteer Scraper for Competitor Portals
**Decision**: Continue using `apify/puppeteer-scraper` alongside new LinkedIn/Indeed scrapers

**Reasoning**:
- Already configured and working (170-180 jobs per run)
- Competitor staffing portals (Insight Global, TEKsystems) not accessible via specialized scrapers
- Custom page function extracts clearance levels and keywords effectively
- Multi-source approach provides comprehensive market coverage

**Alternatives Considered**:
- Replace entirely with LinkedIn scraper
- Rejected because misses important competitor intelligence

---

### Decision 5: Recommend Indeed Scraper as Secondary Priority
**Decision**: Add `valig/indeed-jobs-scraper` as supplement to LinkedIn scraping

**Reasoning**:
- Extremely cheap ($0.0001 per job = $0.05 for 500 jobs)
- 99.9% success rate (highest of all reviewed scrapers)
- Captures smaller staffing agencies that use Indeed over LinkedIn
- Minimal cost means low risk even if data quality lower than LinkedIn

**Alternatives Considered**:
- `borderline/indeed-scraper` - Higher cost ($5/1000) and lower success (90.5%)
- Selected valig for superior reliability and cost efficiency

---

### Decision 6: Document Format as Professional DOCX
**Decision**: Create audit as Word document using docx-js library vs markdown or PDF

**Reasoning**:
- BD team familiar with Word for editing and annotation
- Supports tables, formatting, and page breaks for 40+ page document
- Compatible with corporate document management systems
- Easy to extract sections for presentations

**Alternatives Considered**:
- Markdown (too technical for business audience)
- PDF (not editable by stakeholders)
- Google Docs (requires manual upload)

---

## 3. ARCHITECTURE & DATA FLOW

### Current Architecture (As-Is)
```
Competitor Job Boards (Insight Global, TEKsystems, CACI)
    â†“
apify/puppeteer-scraper (scheduled runs)
    â†“
CSV Export (dataset_puppeteerscraper_*.csv)
    â†“
Manual Import to Notion
    â†“
Notion Program Mapping Intelligence Hub (Status: raw_import)
    â†“
Manual Copy/Paste to ChatGPT GPT-4o
    â†“
Enrichment: Extract location, clearance, program, agency
    â†“
Cross-reference Federal Programs Database (PTS Involvement filtering)
    â†“
ZoomInfo/LinkedIn Contact Discovery (manual search)
    â†“
Import to DCGS Contacts Full Database (manual classification)
    â†“
Claude generates BD Playbooks and Call Sheets
```

**Bottlenecks Identified**:
- Manual CSV import (no API automation)
- Copy/paste to ChatGPT for enrichment
- Contact mapping outside Apify ecosystem
- No historical trend analysis capability

---

### Recommended Architecture (To-Be)
```
Multiple Job Sources:
â”œâ”€â”€ LinkedIn (curious_coder/linkedin-jobs-scraper)
â”œâ”€â”€ Indeed (valig/indeed-jobs-scraper)
â””â”€â”€ Competitor Portals (apify/puppeteer-scraper)
    â†“
Apify Datasets (structured storage with full history)
    â†“
Claude MCP Integration (via get-actor-output tool)
    â†“
AI Enrichment Engine (program mapping, classification)
â”œâ”€â”€ Project Knowledge: Federal Programs database
â”œâ”€â”€ Classification Logic: Program, Tier, Location, Priority
â””â”€â”€ Contact Matching: DCGS Contacts Full cross-reference
    â†“
Notion Databases (via Notion MCP Server)
â”œâ”€â”€ Program Mapping Intelligence Hub (enriched jobs)
â”œâ”€â”€ DCGS Contacts Full (classified contacts)
â””â”€â”€ Federal Programs (reference data)
    â†“
BD Deliverables
â”œâ”€â”€ Weekly HUMINT Updates (DOCX)
â”œâ”€â”€ Call Sheets (XLSX)
â””â”€â”€ BD Playbooks (DOCX)
```

**Key Improvements**:
- Eliminates CSV export/import cycle
- Direct Claude access to Apify datasets
- Automated enrichment using Claude + project knowledge
- Historical analysis via storage tools
- Multi-source aggregation for comprehensive coverage

---

### API Connections & Integrations

#### Apify MCP Server
- **URL**: `https://mcp.apify.com`
- **Authentication**: OAuth (connected via Claude.ai)
- **Rate Limit**: 30 requests/second per user
- **Transport**: Streamable HTTP with OAuth (recommended) or Bearer token

#### Current Tool Configuration
```json
{
  "mcpServers": {
    "apify": {
      "url": "https://mcp.apify.com"
    }
  }
}
```

#### Recommended Enhanced Configuration
```json
{
  "mcpServers": {
    "apify": {
      "url": "https://mcp.apify.com?tools=actors,docs,storage,curious_coder/linkedin-jobs-scraper,valig/indeed-jobs-scraper,apify/puppeteer-scraper"
    }
  }
}
```

**Changes**:
- Adds `storage` category for historical analysis tools
- Pre-loads job scraping Actors for instant availability
- Maintains `actors` and `docs` for dynamic discovery

---

### Data Flow Details

#### Job Scraping Flow
```
1. Actor Execution (3 parallel sources)
   LinkedIn Scraper â†’ LinkedIn Jobs Dataset
   Indeed Scraper â†’ Indeed Jobs Dataset  
   Puppeteer Scraper â†’ Competitor Jobs Dataset

2. Data Retrieval
   Claude calls get-actor-output with datasetId
   Returns: Complete structured job records

3. Enrichment
   Claude processes each job record:
   - Extract program name via pattern matching
   - Map location to DCGS site using classification logic
   - Detect clearance level (Secret, TS, TS/SCI, Poly)
   - Identify role type (analyst, engineer, admin, etc.)
   - Cross-reference Federal Programs database
   - Assign PTS Involvement status (Target/None/Current/Past)

4. Contact Discovery
   For Target programs:
   - Search DCGS Contacts Full for site matches
   - Extract hiring manager names from LinkedIn data
   - Query ZoomInfo exports for new contacts
   - Classify using hierarchy tier logic
   - Calculate BD Priority score

5. Output Generation
   - Update Notion Program Mapping Hub
   - Refresh DCGS Contacts Full with new contacts
   - Generate weekly HUMINT update (DOCX)
   - Create/update BD call sheets (XLSX)
```

#### Webhook Configuration (Future Enhancement)
Not currently implemented, but recommended for n8n automation:

```
Apify Actor Run Complete Webhook
    â†“
n8n Webhook Listener
    â†“
n8n HTTP Request to Apify API (get dataset)
    â†“
n8n Function Node (enrichment logic)
    â†“
n8n Notion Node (update database)
```

---

## 4. CODE & CONFIGURATIONS

### Apify Actor Input Configurations

#### Configuration 1: LinkedIn Jobs Scraper (Recommended)

**File/Component Name**: `curious_coder/linkedin-jobs-scraper`

**Purpose**: Scrape GDIT job postings from LinkedIn with hiring manager contact information

**Actor ID**: `curious_coder/linkedin-jobs-scraper`

**Input Configuration (JSON)**:
```json
{
  "search_query": "intelligence analyst",
  "location": "San Diego, CA",
  "company": "General Dynamics Information Technology",
  "max_results": 50,
  "filters": {
    "job_type": ["Full-time", "Contract"],
    "experience_level": ["Entry level", "Associate", "Mid-Senior level"],
    "easy_apply": false
  }
}
```

**Alternative Search Queries for DCGS Sites**:
```json
// PACAF San Diego
{
  "search_query": "intelligence analyst OR network engineer",
  "location": "San Diego, CA",
  "company": "General Dynamics Information Technology",
  "max_results": 100
}

// Langley AFB (Hampton Roads)
{
  "search_query": "ISR analyst OR cyber security",
  "location": "Hampton, VA",
  "company": "General Dynamics Information Technology", 
  "max_results": 100
}

// Wright-Patterson AFB (Dayton)
{
  "search_query": "radar engineer OR systems administrator",
  "location": "Dayton, OH",
  "company": "General Dynamics Information Technology",
  "max_results": 100
}
```

**Output Schema**:
```json
{
  "job_title": "Intelligence Analyst - TS/SCI",
  "company": "General Dynamics Information Technology",
  "location": "San Diego, CA",
  "job_url": "https://www.linkedin.com/jobs/view/...",
  "posted_date": "2026-01-05",
  "applicants": 47,
  "description": "Full job description text...",
  "salary_range": "$80,000 - $120,000",
  "employment_type": "Full-time",
  "seniority_level": "Mid-Senior level",
  "hiring_manager": {
    "name": "John Smith",
    "title": "Program Manager",
    "linkedin_url": "https://www.linkedin.com/in/..."
  },
  "company_details": {
    "size": "10,001+ employees",
    "industry": "Defense & Space",
    "headquarters": "Falls Church, VA"
  }
}
```

---

#### Configuration 2: Indeed Jobs Scraper

**File/Component Name**: `valig/indeed-jobs-scraper`

**Purpose**: Supplement LinkedIn with broader market coverage from Indeed

**Actor ID**: `valig/indeed-jobs-scraper`

**Input Configuration (JSON)**:
```json
{
  "position": "intelligence analyst",
  "location": "San Diego, CA",
  "maxItems": 100,
  "filters": {
    "jobType": ["fulltime", "contract"],
    "fromAge": 7,
    "radius": 25
  },
  "extendOutputFunction": "({data}) => { return data; }"
}
```

**Output Schema**:
```json
{
  "title": "Intelligence Analyst",
  "company": "General Dynamics",
  "location": "San Diego, CA 92101",
  "salary": "$80,000 - $100,000 a year",
  "description": "Full job description...",
  "url": "https://www.indeed.com/viewjob?jk=...",
  "postedAt": "2 days ago",
  "jobType": "Full-time",
  "benefits": ["Health insurance", "401(k)", "Paid time off"],
  "requirements": {
    "clearance": "TS/SCI required",
    "education": "Bachelor's degree",
    "experience": "3+ years"
  }
}
```

---

#### Configuration 3: Puppeteer Scraper (Current Implementation)

**File/Component Name**: `apify/puppeteer-scraper`

**Purpose**: Scrape competitor staffing agency job boards (Insight Global, TEKsystems, CACI)

**Actor ID**: `apify/puppeteer-scraper`

**Input Configuration (JSON)**:
```json
{
  "startUrls": [
    {
      "url": "https://jobs.insightglobal.com/jobs?keywords=intelligence+analyst&location=San+Diego"
    },
    {
      "url": "https://www.teksystems.com/en/job-seekers/job-search?keyword=network+engineer&location=Hampton+VA"
    }
  ],
  "useChrome": true,
  "proxyConfiguration": {
    "useApifyProxy": true,
    "apifyProxyGroups": ["RESIDENTIAL"]
  },
  "pageFunction": async function pageFunction(context) {
    const { request, log, jQuery: $ } = context;
    
    const jobs = [];
    
    // Extract job listings (adjust selectors based on site)
    $('.job-listing').each(function() {
      const job = {
        company: $(this).find('.company-name').text().trim(),
        title: $(this).find('.job-title').text().trim(),
        location: $(this).find('.location').text().trim(),
        url: $(this).find('a.job-link').attr('href'),
        scraped_at: new Date().toISOString()
      };
      
      // Detect clearance level
      const description = $(this).find('.description').text().toLowerCase();
      if (description.includes('ts/sci') || description.includes('ts-sci')) {
        job.detected_clearance = 'TS/SCI';
      } else if (description.includes('top secret') || description.includes('ts')) {
        job.detected_clearance = 'TS';
      } else if (description.includes('secret')) {
        job.detected_clearance = 'Secret';
      } else {
        job.detected_clearance = 'Unknown';
      }
      
      // Extract primary keyword
      const titleLower = job.title.toLowerCase();
      if (titleLower.includes('analyst')) {
        job.primary_keyword = 'Intelligence Analyst';
      } else if (titleLower.includes('engineer')) {
        job.primary_keyword = 'Network Engineer';
      } else if (titleLower.includes('cyber')) {
        job.primary_keyword = 'Cyber Security';
      } else if (titleLower.includes('admin')) {
        job.primary_keyword = 'Systems Admin';
      } else {
        job.primary_keyword = 'Other';
      }
      
      jobs.push(job);
    });
    
    return jobs;
  },
  "maxRequestRetries": 3,
  "maxConcurrency": 5,
  "maxRequestsPerCrawl": 100
}
```

**Output Schema** (Current CSV Structure):
```csv
company,detected_clearance,location,primary_keyword,scraped_at,title,url
"Insight Global","TS/SCI","San Diego, CA","Intelligence Analyst","2025-12-22T14:08:27.809Z","Intelligence Analyst - TS/SCI Required","https://jobs.insightglobal.com/..."
"TEKsystems","Secret","Hampton, VA","Network Engineer","2025-12-22T14:08:27.809Z","Network Security Engineer","https://www.teksystems.com/..."
```

---

### MCP Tool Usage Examples

#### Example 1: Search for Actors

**Purpose**: Discover available job scraping Actors in Apify Store

**Code/Command**:
```javascript
// Claude MCP tool call
apify:search-actors
{
  "keywords": "linkedin jobs",
  "limit": 10,
  "offset": 0
}
```

**Response Format**:
```json
{
  "results": [
    {
      "id": "curious_coder/linkedin-jobs-scraper",
      "name": "Linkedin Jobs Scraper - PPR",
      "username": "curious_coder",
      "description": "Scrape jobs from linkedin jobs search results along with company details...",
      "url": "https://apify.com/curious_coder/linkedin-jobs-scraper",
      "categories": ["Lead Generation", "Jobs"],
      "pricing": {
        "model": "per_result",
        "price_per_1000": 1.00,
        "currency": "USD"
      },
      "stats": {
        "total_users": 11674,
        "monthly_users": 1235,
        "success_rate": 98.7,
        "bookmarks": 538
      },
      "rating": 4.85
    }
  ]
}
```

---

#### Example 2: Fetch Actor Details

**Purpose**: Get input schema and documentation before running Actor

**Code/Command**:
```javascript
// Claude MCP tool call
apify:fetch-actor-details
{
  "actor": "curious_coder/linkedin-jobs-scraper"
}
```

**Response Format**:
```json
{
  "id": "curious_coder/linkedin-jobs-scraper",
  "title": "Linkedin Jobs Scraper - PPR",
  "description": "Pay per result - Fast and reliable LinkedIn Job Scraper...",
  "readme": "# LinkedIn Jobs Scraper\n\nThis Actor allows you to scrape job listings from LinkedIn...",
  "input_schema": {
    "title": "Input schema",
    "type": "object",
    "properties": {
      "search_query": {
        "title": "Search Query",
        "type": "string",
        "description": "Keywords to search for (e.g., 'software engineer')",
        "editor": "textfield"
      },
      "location": {
        "title": "Location",
        "type": "string",
        "description": "Job location (e.g., 'San Francisco, CA')",
        "editor": "textfield"
      },
      "company": {
        "title": "Company",
        "type": "string",
        "description": "Filter by company name (optional)",
        "editor": "textfield"
      },
      "max_results": {
        "title": "Max Results",
        "type": "integer",
        "description": "Maximum number of job listings to scrape",
        "default": 50,
        "minimum": 1,
        "maximum": 1000
      }
    },
    "required": ["search_query"]
  },
  "pricing": {
    "type": "per_result",
    "tiers": [
      {"level": "FREE", "price_per_1000": 1.00},
      {"level": "BRONZE", "price_per_1000": 1.00}
    ]
  }
}
```

---

#### Example 3: Add Actor as Tool (Dynamic Discovery)

**Purpose**: Register LinkedIn scraper as callable tool in current conversation

**Code/Command**:
```javascript
// Claude MCP tool call
apify:add-actor
{
  "actor": "curious_coder/linkedin-jobs-scraper"
}
```

**Response Format**:
```json
{
  "success": true,
  "message": "Actor curious_coder/linkedin-jobs-scraper added as tool",
  "tool_name": "curious-coder-slash-linkedin-jobs-scraper",
  "input_schema": {
    // Same schema as fetch-actor-details
  }
}
```

**Result**: Actor becomes immediately callable as `curious-coder-slash-linkedin-jobs-scraper` tool

---

#### Example 4: Call Actor (After Adding)

**Purpose**: Execute LinkedIn job scrape for GDIT San Diego positions

**Code/Command**:
```javascript
// Claude MCP tool call (after add-actor)
curious-coder-slash-linkedin-jobs-scraper
{
  "search_query": "intelligence analyst",
  "location": "San Diego, CA",
  "company": "General Dynamics Information Technology",
  "max_results": 50
}
```

**Response Format**:
```json
{
  "actorRunId": "abc123def456",
  "status": "SUCCEEDED",
  "datasetId": "xyz789",
  "defaultDatasetId": "xyz789",
  "startedAt": "2026-01-10T15:30:00.000Z",
  "finishedAt": "2026-01-10T15:32:45.000Z",
  "stats": {
    "itemCount": 47
  },
  "output": {
    "preview": [
      // First 5-10 items shown
      {
        "job_title": "Intelligence Analyst - TS/SCI",
        "company": "General Dynamics Information Technology",
        "location": "San Diego, CA"
      }
    ],
    "message": "Output preview shown. Use get-actor-output tool to retrieve complete dataset."
  }
}
```

---

#### Example 5: Retrieve Complete Output

**Purpose**: Get all 47 jobs from the LinkedIn scrape (not just preview)

**Code/Command**:
```javascript
// Claude MCP tool call
apify:get-actor-output
{
  "datasetId": "xyz789",
  "fields": "job_title,company,location,hiring_manager,description,posted_date",
  "limit": 100,
  "offset": 0
}
```

**Response Format**:
```json
{
  "items": [
    {
      "job_title": "Intelligence Analyst - TS/SCI",
      "company": "General Dynamics Information Technology",
      "location": "San Diego, CA",
      "hiring_manager": {
        "name": "Sarah Johnson",
        "title": "DCGS Program Manager"
      },
      "description": "Full job description...",
      "posted_date": "2026-01-05"
    },
    // ... all 47 items
  ],
  "total": 47,
  "offset": 0,
  "limit": 100,
  "count": 47
}
```

---

#### Example 6: Get Dataset Items (Advanced)

**Purpose**: Advanced filtering and field selection for historical analysis

**Code/Command**:
```javascript
// Claude MCP tool call (requires storage tools enabled)
apify:get-dataset-items
{
  "datasetId": "xyz789",
  "clean": true,
  "desc": false,
  "fields": "job_title,location,posted_date",
  "flatten": "hiring_manager",
  "limit": 50,
  "offset": 0
}
```

**Response Format**:
```json
{
  "items": [
    {
      "job_title": "Intelligence Analyst - TS/SCI",
      "location": "San Diego, CA",
      "posted_date": "2026-01-05",
      "hiring_manager.name": "Sarah Johnson",
      "hiring_manager.title": "DCGS Program Manager"
    }
  ],
  "total": 47,
  "offset": 0,
  "limit": 50,
  "count": 47
}
```

**Note**: `flatten` parameter transforms nested objects into dot-notation fields

---

#### Example 7: List Historical Datasets

**Purpose**: Find all past job scraping runs for trend analysis

**Code/Command**:
```javascript
// Claude MCP tool call (requires storage tools enabled)
apify:get-dataset-list
{
  "unnamed": false,
  "limit": 50,
  "desc": true
}
```

**Response Format**:
```json
{
  "items": [
    {
      "id": "xyz789",
      "name": "linkedin-jobs-2026-01-10",
      "createdAt": "2026-01-10T15:30:00.000Z",
      "modifiedAt": "2026-01-10T15:32:45.000Z",
      "itemCount": 47,
      "fields": ["job_title", "company", "location", "hiring_manager", "description"]
    },
    {
      "id": "abc123",
      "name": "puppeteer-scraper-2026-01-03",
      "createdAt": "2026-01-03T10:15:00.000Z",
      "itemCount": 174
    }
  ],
  "total": 15,
  "offset": 0,
  "limit": 50,
  "count": 15
}
```

---

### Classification Logic (Python)

**File/Component Name**: `classification_logic.py`

**Purpose**: Classify DCGS contacts by hierarchy tier, program, location hub, and BD priority

**Code**:
```python
#!/usr/bin/env python3
"""
DCGS Contact Classification Logic
Used to classify contacts in Notion DCGS Contacts Full database
"""

def get_hierarchy_tier(title: str) -> str:
    """
    Determine hierarchy tier from job title
    
    Args:
        title: Job title string
        
    Returns:
        One of: Tier 1-6 classifications
    """
    title_lower = title.lower()
    
    # Tier 1 - Executive
    tier1_keywords = ['vice president', 'vp,', 'vp ', 'president', 
                      'chief ', 'ceo', 'cto', 'cio', 'ciso', 'cfo']
    if any(kw in title_lower for kw in tier1_keywords):
        return 'Tier 1 - Executive'
    
    # Tier 2 - Director
    if 'director' in title_lower:
        return 'Tier 2 - Director'
    
    # Tier 3 - Program Leadership
    tier3_keywords = ['program manager', 'deputy program', 'site lead', 
                      'task order', 'program director', 'project director']
    if any(kw in title_lower for kw in tier3_keywords):
        return 'Tier 3 - Program Leadership'
    
    # Tier 4 - Management
    tier4_keywords = ['manager', 'team lead', 'supervisor', 
                      'lead,', 'lead ', 'section chief']
    if any(kw in title_lower for kw in tier4_keywords):
        return 'Tier 4 - Management'
    
    # Tier 5 - Senior IC
    tier5_keywords = ['senior ', 'sr.', 'sr ', 'principal', 
                      'lead engineer', 'staff engineer', 'architect']
    if any(kw in title_lower for kw in tier5_keywords):
        return 'Tier 5 - Senior IC'
    
    # Tier 6 - Individual Contributor (default)
    return 'Tier 6 - Individual Contributor'


def get_program_from_location(location: str) -> str:
    """
    Map location to DCGS program
    
    Args:
        location: City or base name
        
    Returns:
        Program name or 'Unknown'
    """
    LOCATION_TO_PROGRAM = {
        # AF DCGS - Langley (DGS-1)
        'Hampton': 'AF DCGS - Langley',
        'Newport News': 'AF DCGS - Langley',
        'Langley': 'AF DCGS - Langley',
        'Yorktown': 'AF DCGS - Langley',
        
        # AF DCGS - PACAF (San Diego) ðŸ”¥
        'San Diego': 'AF DCGS - PACAF',
        'La Mesa': 'AF DCGS - PACAF',
        
        # AF DCGS - Wright-Patterson
        'Dayton': 'AF DCGS - Wright-Patt',
        'Beavercreek': 'AF DCGS - Wright-Patt',
        'Fairborn': 'AF DCGS - Wright-Patt',
        
        # Navy DCGS-N
        'Norfolk': 'Navy DCGS-N',
        'Suffolk': 'Navy DCGS-N',
        'Chesapeake': 'Navy DCGS-N',
        'Virginia Beach': 'Navy DCGS-N',
        'Tracy': 'Navy DCGS-N',
        
        # Army DCGS-A
        'Fort Belvoir': 'Army DCGS-A',
        'Fort Detrick': 'Army DCGS-A',
        'Aberdeen': 'Army DCGS-A',
        
        # Corporate HQ
        'Herndon': 'Corporate HQ',
        'Falls Church': 'Corporate HQ',
        'Reston': 'Corporate HQ',
        'Fairfax': 'Corporate HQ',
        'Chantilly': 'Corporate HQ'
    }
    
    return LOCATION_TO_PROGRAM.get(location, 'Unknown')


def get_location_hub(location: str) -> str:
    """
    Map location to geographic hub
    
    Args:
        location: City or base name
        
    Returns:
        Location hub name
    """
    LOCATION_TO_HUB = {
        # Hampton Roads
        'Hampton': 'Hampton Roads',
        'Newport News': 'Hampton Roads',
        'Langley': 'Hampton Roads',
        'Yorktown': 'Hampton Roads',
        'Norfolk': 'Hampton Roads',
        'Virginia Beach': 'Hampton Roads',
        'Chesapeake': 'Hampton Roads',
        'Suffolk': 'Hampton Roads',
        
        # San Diego Metro
        'San Diego': 'San Diego Metro',
        'La Mesa': 'San Diego Metro',
        
        # DC Metro
        'Herndon': 'DC Metro',
        'Falls Church': 'DC Metro',
        'Reston': 'DC Metro',
        'Fairfax': 'DC Metro',
        'Chantilly': 'DC Metro',
        
        # Dayton/Wright-Patt
        'Dayton': 'Dayton/Wright-Patt',
        'Beavercreek': 'Dayton/Wright-Patt',
        'Fairborn': 'Dayton/Wright-Patt',
        
        # Other CONUS
        'Fort Belvoir': 'Other CONUS',
        'Fort Detrick': 'Other CONUS',
        'Aberdeen': 'Other CONUS',
        'Tracy': 'Other CONUS'
    }
    
    return LOCATION_TO_HUB.get(location, 'Unknown')


def get_bd_priority(hierarchy_tier: str, program: str = None) -> str:
    """
    Calculate BD priority score
    
    Args:
        hierarchy_tier: Contact's tier classification
        program: Optional program assignment
        
    Returns:
        BD Priority emoji classification
    """
    # Critical: Executives + PACAF site
    if hierarchy_tier in ['Tier 1 - Executive', 'Tier 2 - Director']:
        return 'ðŸ”´ Critical'
    if program == 'AF DCGS - PACAF':
        return 'ðŸ”´ Critical'  # San Diego is priority target
    
    # High: Program Managers and Site Leads
    if hierarchy_tier == 'Tier 3 - Program Leadership':
        return 'ðŸŸ  High'
    
    # Medium: Managers
    if hierarchy_tier == 'Tier 4 - Management':
        return 'ðŸŸ¡ Medium'
    
    # Standard: ICs
    return 'âšª Standard'


def get_functional_area(title: str) -> list:
    """
    Extract functional areas from job title
    
    Args:
        title: Job title string
        
    Returns:
        List of functional area tags
    """
    title_lower = title.lower()
    areas = []
    
    if any(kw in title_lower for kw in ['program manager', 'pm', 'project manager']):
        areas.append('Program Management')
    
    if any(kw in title_lower for kw in ['network', 'communications']):
        areas.append('Network Engineering')
    
    if any(kw in title_lower for kw in ['cyber', 'security', 'infosec']):
        areas.append('Cyber Security')
    
    if any(kw in title_lower for kw in ['intelligence', 'analyst', 'isr']):
        areas.append('ISR/Intelligence')
    
    if any(kw in title_lower for kw in ['system admin', 'sysadmin', 'administrator']):
        areas.append('Systems Administration')
    
    if any(kw in title_lower for kw in ['software', 'developer', 'engineer']):
        areas.append('Software Engineering')
    
    if any(kw in title_lower for kw in ['field service', 'field support']):
        areas.append('Field Service')
    
    if any(kw in title_lower for kw in ['fso', 'security officer', 'facility security']):
        areas.append('Security/FSO')
    
    if any(kw in title_lower for kw in ['business development', 'bd', 'capture']):
        areas.append('Business Development')
    
    if any(kw in title_lower for kw in ['training', 'instructor']):
        areas.append('Training')
    
    if any(kw in title_lower for kw in ['admin', 'administrative', 'assistant']):
        areas.append('Administrative')
    
    return areas if areas else ['Other']


def classify_contact(name: str, title: str, location: str) -> dict:
    """
    Complete contact classification
    
    Args:
        name: Contact name
        title: Job title
        location: Work location
        
    Returns:
        Dictionary with all classification fields
    """
    hierarchy_tier = get_hierarchy_tier(title)
    program = get_program_from_location(location)
    location_hub = get_location_hub(location)
    bd_priority = get_bd_priority(hierarchy_tier, program)
    functional_areas = get_functional_area(title)
    
    return {
        'name': name,
        'title': title,
        'location': location,
        'hierarchy_tier': hierarchy_tier,
        'program': program,
        'location_hub': location_hub,
        'bd_priority': bd_priority,
        'functional_areas': functional_areas
    }


# Example usage
if __name__ == '__main__':
    # Test PACAF critical contact
    contact1 = classify_contact(
        name='Kingsley Ero',
        title='Acting Site Lead',
        location='San Diego'
    )
    print(f"\nContact 1: {contact1}")
    # Expected: Tier 3, AF DCGS - PACAF, Critical priority
    
    # Test Langley PM
    contact2 = classify_contact(
        name='Maureen Shamaly',
        title='Program Manager',
        location='Hampton'
    )
    print(f"\nContact 2: {contact2}")
    # Expected: Tier 3, AF DCGS - Langley, High priority
    
    # Test executive
    contact3 = classify_contact(
        name='David Winkelman',
        title='Vice President, Defense Intelligence',
        location='Herndon'
    )
    print(f"\nContact 3: {contact3}")
    # Expected: Tier 1, Corporate HQ, Critical priority
```

---

### Job Enrichment Logic (Python)

**File/Component Name**: `job_enrichment.py`

**Purpose**: Extract program names, locations, and clearances from job descriptions

**Code**:
```python
#!/usr/bin/env python3
"""
Job Posting Enrichment Logic
Extracts program, location, clearance, and role information from job descriptions
"""

import re
from typing import Dict, Optional, List

# Known DCGS program indicators
PROGRAM_KEYWORDS = {
    'AF DCGS': [
        'af dcgs', 'air force dcgs', 'dgs-1', 'dgs-2', 'dgs-3', 'dgs-4', 'dgs-5',
        'distributed common ground system', 'dcgs air force',
        '480th isrw', '480th intelligence', 'nasic'
    ],
    'Army DCGS-A': [
        'dcgs-a', 'dcgs army', 'army dcgs', 'distributed common ground system army',
        'trojan', 'dcgs-a increment'
    ],
    'Navy DCGS-N': [
        'dcgs-n', 'dcgs navy', 'navy dcgs', 'distributed common ground system navy',
        'dcgs-n increment'
    ]
}

# Location patterns
LOCATION_PATTERNS = {
    'San Diego': ['san diego', 'la mesa'],
    'Hampton Roads': ['hampton', 'newport news', 'langley afb', 'norfolk', 'virginia beach', 'chesapeake'],
    'Dayton': ['dayton', 'wright-patterson', 'wright patterson', 'wpafb', 'beavercreek', 'fairborn'],
    'DC Metro': ['herndon', 'falls church', 'reston', 'fairfax', 'chantilly', 'mclean'],
    'Fort Belvoir': ['fort belvoir', 'belvoir'],
    'Fort Detrick': ['fort detrick', 'detrick'],
    'Aberdeen': ['aberdeen proving ground', 'apg'],
    'Tracy': ['tracy', 'ca']
}

# Clearance patterns
CLEARANCE_PATTERNS = {
    'TS/SCI with Poly': [r'ts/sci.*poly', r'top secret.*polygraph', r'ts-sci.*poly'],
    'TS/SCI': [r'ts/sci', r'ts-sci', r'top secret.*sci'],
    'TS': [r'\bts\b', r'top secret(?!.*sci)'],
    'Secret': [r'\bsecret\b(?!.*top)']
}

# Role type patterns
ROLE_TYPES = {
    'Intelligence Analyst': ['intelligence analyst', 'isr analyst', 'all-source analyst', 'geoint analyst', 'sigint analyst'],
    'Network Engineer': ['network engineer', 'network administrator', 'communications engineer'],
    'Cyber Security': ['cyber security', 'cybersecurity', 'information security', 'infosec', 'security engineer'],
    'Systems Administrator': ['systems administrator', 'sysadmin', 'system admin', 'linux admin', 'windows admin'],
    'Software Engineer': ['software engineer', 'software developer', 'programmer', 'devops'],
    'Program Manager': ['program manager', 'project manager', 'pm'],
    'Field Service': ['field service', 'field support', 'field engineer'],
    'Radar Engineer': ['radar engineer', 'radar analyst', 'signals analyst']
}


def extract_program(text: str) -> Optional[str]:
    """
    Extract DCGS program name from job description
    
    Args:
        text: Job title + description combined
        
    Returns:
        Program name or None
    """
    text_lower = text.lower()
    
    for program, keywords in PROGRAM_KEYWORDS.items():
        if any(kw in text_lower for kw in keywords):
            return program
    
    return None


def extract_location(text: str) -> Optional[str]:
    """
    Extract location from job posting
    
    Args:
        text: Job location field + description
        
    Returns:
        Standardized location name or None
    """
    text_lower = text.lower()
    
    for location, patterns in LOCATION_PATTERNS.items():
        if any(pattern in text_lower for pattern in patterns):
            return location
    
    return None


def extract_clearance(text: str) -> Optional[str]:
    """
    Detect security clearance requirement
    
    Args:
        text: Job description text
        
    Returns:
        Clearance level or None
    """
    text_lower = text.lower()
    
    # Check in order of specificity (most specific first)
    for clearance, patterns in CLEARANCE_PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, text_lower):
                return clearance
    
    return None


def extract_role_type(title: str) -> Optional[str]:
    """
    Categorize role type from job title
    
    Args:
        title: Job title
        
    Returns:
        Role category or None
    """
    title_lower = title.lower()
    
    for role, keywords in ROLE_TYPES.items():
        if any(kw in title_lower for kw in keywords):
            return role
    
    return 'Other'


def extract_agency(text: str) -> str:
    """
    Determine government agency (Air Force, Army, Navy)
    
    Args:
        text: Job description
        
    Returns:
        Agency name
    """
    text_lower = text.lower()
    
    # Air Force indicators
    af_keywords = ['air force', 'usaf', 'af ', 'afb', 'pacaf', '480th', 'nasic']
    if any(kw in text_lower for kw in af_keywords):
        return 'Air Force'
    
    # Army indicators
    army_keywords = ['army', 'usa ', 'fort belvoir', 'fort detrick', 'apg']
    if any(kw in text_lower for kw in army_keywords):
        return 'Army'
    
    # Navy indicators
    navy_keywords = ['navy', 'usn', 'naval', 'norfolk', 'navsup']
    if any(kw in text_lower for kw in navy_keywords):
        return 'Navy'
    
    return 'Unknown'


def enrich_job(job: Dict) -> Dict:
    """
    Complete job enrichment pipeline
    
    Args:
        job: Raw job dictionary with title, description, location, company
        
    Returns:
        Enriched job dictionary with extracted fields
    """
    # Combine text for analysis
    full_text = f"{job.get('title', '')} {job.get('description', '')} {job.get('location', '')}"
    
    enriched = {
        **job,  # Preserve original fields
        'program_name': extract_program(full_text),
        'standardized_location': extract_location(full_text),
        'clearance_required': extract_clearance(job.get('description', '')),
        'role_type': extract_role_type(job.get('title', '')),
        'agency': extract_agency(full_text)
    }
    
    # Calculate BD priority based on program and location
    if enriched['standardized_location'] == 'San Diego':
        enriched['bd_priority'] = 'Critical'
    elif enriched['program_name'] in ['AF DCGS', 'Army DCGS-A', 'Navy DCGS-N']:
        enriched['bd_priority'] = 'High'
    else:
        enriched['bd_priority'] = 'Medium'
    
    return enriched


def batch_enrich_jobs(jobs: List[Dict]) -> List[Dict]:
    """
    Enrich multiple jobs
    
    Args:
        jobs: List of raw job dictionaries
        
    Returns:
        List of enriched job dictionaries
    """
    return [enrich_job(job) for job in jobs]


# Example usage
if __name__ == '__main__':
    # Test job from LinkedIn scraper
    sample_job = {
        'title': 'Intelligence Analyst - TS/SCI Required',
        'company': 'General Dynamics Information Technology',
        'location': 'San Diego, CA',
        'description': '''
        GDIT is seeking an Intelligence Analyst to support AF DCGS operations
        at the PACAF node in San Diego. Must possess active TS/SCI clearance.
        Experience with DCGS-A systems preferred. Support 480th ISRW mission.
        '''
    }
    
    enriched = enrich_job(sample_job)
    
    print("\nOriginal Job:")
    print(f"Title: {sample_job['title']}")
    print(f"Location: {sample_job['location']}")
    
    print("\nEnriched Fields:")
    print(f"Program: {enriched['program_name']}")
    print(f"Standardized Location: {enriched['standardized_location']}")
    print(f"Clearance: {enriched['clearance_required']}")
    print(f"Role Type: {enriched['role_type']}")
    print(f"Agency: {enriched['agency']}")
    print(f"BD Priority: {enriched['bd_priority']}")
```

---

## 5. NOTION DATABASE SCHEMAS

### Database 1: DCGS Contacts Full

**Database Name**: DCGS Contacts Full  
**Collection ID**: `2ccdef65-baa5-8087-a53b-000ba596128e`  
**Database URL**: https://www.notion.so/2ccdef65baa580d09b66c67d66e7a54d  
**Purpose**: Primary BD contact database for DCGS program personnel

**Properties**:

| Property Name | Type | Options/Configuration |
|--------------|------|----------------------|
| Name | Title | - |
| First Name | Text | - |
| Last Name | Text | - |
| Job Title | Text | - |
| Email Address | Email | - |
| Phone Number | Phone | - |
| Direct Phone Number | Phone | - |
| Mobile phone | Phone | - |
| LinkedIn Contact Profile URL | URL | - |
| Person City | Text | - |
| Person State | Text | - |
| Program | Select | Options: AF DCGS - Langley, AF DCGS - Wright-Patt, AF DCGS - PACAF, AF DCGS - Other, Army DCGS-A, Navy DCGS-N, Corporate HQ, Enterprise Security, Unassigned |
| Hierarchy Tier | Select | Options: Tier 1 - Executive, Tier 2 - Director, Tier 3 - Program Leadership, Tier 4 - Management, Tier 5 - Senior IC, Tier 6 - Individual Contributor |
| BD Priority | Select | Options: ðŸ”´ Critical, ðŸŸ  High, ðŸŸ¡ Medium, âšª Standard |
| Location Hub | Select | Options: Hampton Roads, San Diego Metro, DC Metro, Dayton/Wright-Patt, Other CONUS, OCONUS, Unknown |
| Functional Area | Multi-select | Options: Program Management, Network Engineering, Cyber Security, ISR/Intelligence, Systems Administration, Software Engineering, Field Service, Security/FSO, Business Development, Training, Administrative |
| Notes | Text (long) | - |
| Source | Select | Options: ZoomInfo, LinkedIn, Manual Entry, Web Scraping, Referral |
| Last Contact Date | Date | - |
| Next Follow-Up | Date | - |
| Status | Select | Options: New Lead, Contacted, In Discussion, Qualified, Not Interested, Invalid |

**Relations**:
- None (standalone database)

**Formulas/Rollups**:
- None currently configured

---

### Database 2: Program Mapping Intelligence Hub

**Database Name**: Program Mapping Intelligence Hub  
**Collection ID**: `f57792c1-605b-424c-8830-23ab41c47137`  
**Database URL**: https://www.notion.so/0a0d7e463d8840b6853a3c9680347644  
**Purpose**: Enriched job scraping results with program mapping and BD scoring

**Properties**:

| Property Name | Type | Options/Configuration |
|--------------|------|----------------------|
| Job Title | Title | - |
| Company | Text | - |
| Location | Text | - |
| Standardized Location | Select | Options: San Diego, Hampton Roads, Dayton, DC Metro, Fort Belvoir, Fort Detrick, Aberdeen, Tracy, Other, Unknown |
| Detected Clearance | Select | Options: TS/SCI with Poly, TS/SCI, TS, Secret, Unknown |
| Primary Keyword | Text | - |
| Role Type | Select | Options: Intelligence Analyst, Network Engineer, Cyber Security, Systems Administrator, Software Engineer, Program Manager, Field Service, Radar Engineer, Other |
| Program Name | Text | - |
| Matched Program | Select | Options: AF DCGS - Langley, AF DCGS - Wright-Patt, AF DCGS - PACAF, AF DCGS - Other, Army DCGS-A, Navy DCGS-N, Other GDIT, Non-GDIT |
| Agency | Select | Options: Air Force, Army, Navy, Other DoD, IC, Unknown |
| Job URL | URL | - |
| Scraped At | Date | - |
| Description | Text (long) | - |
| Status | Select | Options: raw_import, pending_enrichment, enriching, enriched, validated, archived |
| BD Priority Score | Number | 1-5 scale |
| PTS Involvement | Select | Options: Current, Past, Target, None |

**Relations**:
- Linked to Federal Programs database (via Matched Program)

**Formulas/Rollups**:
- **BD Priority Score** (Formula): 
  ```
  if(prop("Standardized Location") == "San Diego", 5, 
     if(prop("Matched Program") contains "DCGS", 4, 
        if(prop("PTS Involvement") == "Target", 3, 2)))
  ```

---

### Database 3: Federal Programs

**Database Name**: Federal Programs  
**Collection ID**: `06cd9b22-5d6b-4d37-b0d3-ba99da4971fa`  
**Database URL**: https://www.notion.so/9db40fce078142b9902cd4b0263b1e23  
**Purpose**: Master reference for all federal programs tracked by PTS

**Properties**:

| Property Name | Type | Options/Configuration |
|--------------|------|----------------------|
| Program Name | Title | - |
| Acronym | Text | - |
| Agency Owner | Select | Options: Air Force, Army, Navy, DIA, NSA, NGA, Other DoD, IC |
| Prime Contractor | Select | Options: GDIT, Leidos, SAIC, Peraton, CACI, BAE Systems, Northrop Grumman, Raytheon, Other |
| Known Subcontractors | Text | Comma-separated list |
| Contract Value | Text | Dollar range (e.g., "$500M - $1B") |
| Contract Vehicle/Type | Select | Options: IDIQ, Task Order, FFP, CPFF, T&M, Other |
| PoP Start | Date | - |
| PoP End | Date | - |
| Key Locations | Text | Comma-separated cities/bases |
| Clearance Requirements | Select | Options: TS/SCI with Poly, TS/SCI, TS, Secret, Multiple Levels |
| Typical Roles | Text | Comma-separated role types |
| Keywords/Signals | Text | Job description keywords indicating this program |
| Program Type | Select | Options: ISR, C4I, Cyber, IT Services, Engineering, Other |
| Source Evidence | URL | Link to contract award, job posting, or reference |
| Confidence Level | Select | Options: High (verified), Medium (inferred), Low (unconfirmed) |
| PTS Involvement | Select | Options: Current (active staff), Past (former staff), Target (pursuing), None |
| Priority Level | Select | Options: Critical, High, Medium, Low |
| Pain Points | Text (long) | Known staffing challenges |
| Past Performance | Text (long) | PTS placement history |

**Relations**:
- Linked from Program Mapping Intelligence Hub (via Matched Program field)

**Formulas/Rollups**:
- None currently configured

---

### Database 4: GDIT Jobs

**Database Name**: GDIT Jobs  
**Collection ID**: `2ccdef65-baa5-80b0-9a80-000bd2745f63`  
**Database URL**: https://www.notion.so/2ccdef65baa580669cb6ee688ede23f4  
**Purpose**: Current GDIT job openings from Bullhorn ATS

**Properties**:

| Property Name | Type | Options/Configuration |
|--------------|------|----------------------|
| Job Title | Title | - |
| Date Added | Date | - |
| Owner | Text | BD team member responsible |
| Contact | Text | Hiring manager name |
| Employment Type | Select | Options: Full-time, Contract, Temp-to-Perm |
| Open/Closed | Select | Options: Open, Closed, On Hold |
| Status | Select | Options: Active, Filled, Cancelled |
| Pay Rate | Number | Hourly rate |
| Client Bill Rate | Number | Billing rate |
| Salary | Number | Annual salary |
| Perm Fee (%) | Number | Percentage for permanent placements |

**Relations**:
- None (standalone database)

**Formulas/Rollups**:
- None currently configured

---

## 6. N8N WORKFLOWS

### Workflow Status: Not Currently Implemented

**Note**: No n8n workflows were created during this conversation. However, the audit document recommends future n8n integration for automation.

### Recommended Future Workflow: Weekly Job Aggregation

**Workflow Name**: DCGS Weekly Job Aggregation  
**Purpose**: Automated weekly execution of all job scrapers with Notion database updates

**Planned Node Sequence**:

```
Schedule Trigger (Mondays 6:00 AM)
    â†“
HTTP Request â†’ Apify API: Run LinkedIn Scraper
    â†“
Wait 5 minutes
    â†“
HTTP Request â†’ Apify API: Get LinkedIn Dataset
    â†“
HTTP Request â†’ Apify API: Run Indeed Scraper
    â†“
Wait 5 minutes
    â†“
HTTP Request â†’ Apify API: Get Indeed Dataset
    â†“
HTTP Request â†’ Apify API: Run Puppeteer Scraper
    â†“
Wait 5 minutes
    â†“
HTTP Request â†’ Apify API: Get Puppeteer Dataset
    â†“
Function Node: Merge & Deduplicate Jobs
    â†“
Function Node: Enrich with Classification Logic
    â†“
Notion Node: Bulk Insert to Program Mapping Hub
    â†“
Send Email: Weekly Report Summary
```

**Key Configurations** (Planned):

**Schedule Trigger**:
```json
{
  "rule": {
    "interval": [
      {
        "field": "cronExpression",
        "expression": "0 6 * * 1"
      }
    ]
  }
}
```

**HTTP Request - Run LinkedIn Scraper**:
```json
{
  "method": "POST",
  "url": "https://api.apify.com/v2/acts/curious_coder~linkedin-jobs-scraper/runs",
  "authentication": "genericCredentialType",
  "genericAuthType": "httpHeaderAuth",
  "sendHeaders": true,
  "headerParameters": {
    "parameters": [
      {
        "name": "Authorization",
        "value": "Bearer {{$credentials.apifyApiToken}}"
      }
    ]
  },
  "sendBody": true,
  "bodyParameters": {
    "parameters": [
      {
        "name": "search_query",
        "value": "intelligence analyst OR network engineer"
      },
      {
        "name": "location",
        "value": "San Diego, CA"
      },
      {
        "name": "company",
        "value": "General Dynamics Information Technology"
      },
      {
        "name": "max_results",
        "value": 100
      }
    ]
  }
}
```

**Webhook URLs/Credentials**:
- Not yet configured (future implementation)

---

## 7. APIFY ACTORS & SCRAPERS

### Actor 1: LinkedIn Jobs Scraper (Recommended)

**Actor ID**: `curious_coder/linkedin-jobs-scraper`  
**Actor URL**: https://apify.com/curious_coder/linkedin-jobs-scraper  
**Status**: Not yet added (recommended for immediate implementation)

**Input Configuration**:
```json
{
  "search_query": "intelligence analyst OR network engineer OR cyber security",
  "location": "San Diego, CA",
  "company": "General Dynamics Information Technology",
  "max_results": 100,
  "filters": {
    "job_type": ["Full-time", "Contract"],
    "experience_level": ["Entry level", "Associate", "Mid-Senior level"],
    "easy_apply": false
  }
}
```

**Output Schema**:
- See Code & Configurations section for complete schema

**Rate Limiting**:
- Apify platform: 30 requests/second
- Actor-specific: No additional limits
- Recommended: 1 run per site per week

**Scheduling Settings**:
- Frequency: Weekly (Mondays 6:00 AM)
- Timeout: 5 minutes
- Max retries: 3

**Pricing**:
- Model: Pay Per Result
- Cost: $1 per 1,000 jobs
- Estimated monthly cost: ~$2 (500 jobs/week Ã— 4 weeks = 2,000 jobs)

---

### Actor 2: Indeed Jobs Scraper (Recommended)

**Actor ID**: `valig/indeed-jobs-scraper`  
**Actor URL**: https://apify.com/valig/indeed-jobs-scraper  
**Status**: Not yet added (recommended as supplement)

**Input Configuration**:
```json
{
  "position": "intelligence analyst",
  "location": "San Diego, CA",
  "maxItems": 100,
  "filters": {
    "jobType": ["fulltime", "contract"],
    "fromAge": 7,
    "radius": 25
  }
}
```

**Output Schema**:
- See Code & Configurations section for complete schema

**Rate Limiting**:
- Same as LinkedIn scraper

**Scheduling Settings**:
- Frequency: Weekly (Mondays 6:30 AM - offset from LinkedIn)
- Timeout: 5 minutes
- Max retries: 3

**Pricing**:
- Model: Pay Per Event (tiered)
- Cost: $0.0001 per job (FREE tier)
- Estimated monthly cost: ~$0.20 (500 jobs/week Ã— 4 weeks Ã— $0.0001)

---

### Actor 3: Puppeteer Scraper (Currently Active)

**Actor ID**: `apify/puppeteer-scraper`  
**Actor URL**: https://apify.com/apify/puppeteer-scraper  
**Status**: Active (in use since December 2025)

**Input Configuration**:
- See Code & Configurations section for complete pageFunction code

**Output Schema**:
```csv
company,detected_clearance,location,primary_keyword,scraped_at,title,url
```

**Recent Run History**:
- December 17, 2025: 175 jobs
- December 19, 2025: 178 jobs
- December 22, 2025: 174 jobs
- Average: ~175 jobs/run

**Rate Limiting**:
- Apify platform: 30 requests/second
- Configured max concurrency: 5
- Max requests per crawl: 100

**Scheduling Settings**:
- Current: Manual trigger
- Recommended: Weekly (Mondays 7:00 AM)

**Pricing**:
- Model: Platform usage (compute time)
- Estimated cost: $0.50-1.00 per run (5-10 minutes Ã— platform rates)

---

### Actor 4: RAG Web Browser (Pre-loaded)

**Actor ID**: `apify/rag-web-browser`  
**Actor URL**: https://apify.com/apify/rag-web-browser  
**Status**: Pre-loaded in MCP configuration (enabled by default)

**Purpose**: General web browsing and data extraction for AI agents

**Input Configuration**:
```json
{
  "url": "https://www.gdit.com/careers",
  "maxCrawlDepth": 1,
  "extractText": true,
  "extractLinks": true
}
```

**Use Case for DCGS Campaign**: 
- Extract GDIT career page content for job discovery
- Scrape prime contractor news/announcements
- Retrieve program-specific web pages

---

## 8. PROBLEMS SOLVED

### Problem 1: Understanding MCP Server Capabilities

**Problem Description**:  
User requested comprehensive audit of Apify MCP Server but unclear what tools were available, how to use them, and which were most relevant for job scraping automation.

**Root Cause**:  
- No centralized documentation of all available MCP tools
- User unfamiliar with Model Context Protocol standard
- Previous interactions focused on specific tasks, not overall capabilities

**Solution Implemented**:
1. Searched Apify documentation using `apify:search-apify-docs` tool
2. Fetched complete MCP integration guide using `apify:fetch-apify-docs`
3. Tested `apify:search-actors` tool with multiple queries
4. Documented all 20+ core tools with parameters and use cases
5. Created 40+ page DOCX audit document with examples

**Verification**:
- Document includes complete tool catalog
- Input parameters documented for each tool
- Use cases and examples provided
- Constraints and limitations noted (rate limits, authentication)

---

### Problem 2: Identifying Optimal Job Scraping Actors

**Problem Description**:  
Current Puppeteer scraper working but may not be optimal. Need to evaluate alternatives for LinkedIn and Indeed scraping.

**Root Cause**:  
- 1,000+ Actors in Apify Store, unclear which are best for defense contractor job scraping
- No systematic evaluation of pricing, success rates, and features
- User manually scraping competitor portals without knowing specialized tools exist

**Solution Implemented**:
1. Searched for "linkedin jobs" actors - found 10 options
2. Searched for "indeed jobs" actors - found 5 options
3. Compared pricing models (per-result vs per-event vs free)
4. Evaluated success rates and user adoption (11K+ users validates reliability)
5. Recommended `curious_coder/linkedin-jobs-scraper` as priority #1
6. Recommended `valig/indeed-jobs-scraper` as low-cost supplement
7. Documented input schemas and output structures

**Verification**:
- Top 3 actors for each platform documented
- Pricing comparison table created
- Success rates and user counts verified
- Input/output schemas documented

---

### Problem 3: Manual CSV Import/Export Bottleneck

**Problem Description**:  
Current workflow requires exporting CSV from Apify, manually importing to Notion, copy/pasting to ChatGPT for enrichment. Time-consuming and error-prone.

**Root Cause**:  
- User unaware that `get-actor-output` tool provides direct access to datasets
- Storage tools disabled (couldn't browse historical data)
- No integration between Apify datasets and Claude enrichment workflow

**Solution Implemented**:
1. Documented `get-actor-output` tool for direct dataset retrieval
2. Recommended enabling storage tools category
3. Designed new architecture: Apify â†’ Claude MCP â†’ AI Enrichment â†’ Notion
4. Provided Python classification logic that can be integrated into Claude prompts
5. Calculated ROI: $13K-18K annual savings from automation

**Verification**:
- Architecture diagram shows automated flow
- Classification logic provided as reusable code
- Storage tools documented with examples
- ROI calculation shows 430x-599x return

---

### Problem 4: No Historical Trend Analysis

**Problem Description**:  
Current workflow only looks at current week's jobs. Cannot detect hiring surges (signals contract option exercises) or compare competitor activity over time.

**Root Cause**:  
- Storage tools disabled by default in MCP configuration
- User didn't know `get-dataset-list` and `get-dataset-items` tools existed
- CSV exports stored locally but not queryable

**Solution Implemented**:
1. Documented all storage tools (8 total)
2. Explained `get-dataset-list` retrieves all past scraping runs
3. Showed `get-dataset-items` supports advanced filtering and pagination
4. Recommended weekly comparison workflow
5. Provided use case: "20%+ increase in PACAF jobs = option year pickup signal"

**Verification**:
- Storage tools section complete in audit document
- Example queries for trend analysis provided
- Week-over-week workflow documented in recommendations

---

### Problem 5: LinkedIn Hiring Manager Data Not Captured

**Problem Description**:  
Current Puppeteer scraper gets job titles but no hiring manager names or contact info. LinkedIn has this data but not being collected.

**Root Cause**:  
- Puppeteer scraping generic job boards (Insight Global, TEKsystems)
- These sites don't show hiring manager details
- User didn't know LinkedIn-specific scrapers extract contact info

**Solution Implemented**:
1. Identified `curious_coder/linkedin-jobs-scraper` extracts hiring manager names
2. Documented output schema includes hiring_manager.name and hiring_manager.title
3. Showed how this integrates with contact discovery workflow
4. Calculated this eliminates ZoomInfo lookup step (saves 2-3 hours/week)

**Verification**:
- LinkedIn scraper output schema documented
- Hiring manager fields highlighted
- Integration with DCGS Contacts Full explained
- Time savings quantified

---

### Problem 6: No Cost-Benefit Analysis

**Problem Description**:  
User investing time in manual BD intelligence gathering but unclear if automation worth the investment.

**Root Cause**:  
- No baseline measurement of current labor costs
- Apify pricing models complex (per-result, per-event, tiered)
- Uncertain if automation would actually save time

**Solution Implemented**:
1. Estimated current manual process: 8-10 hours/week analyst time
2. Calculated labor cost: $400-500/week at $50/hr blended rate
3. Priced automation: $0.55/week in scraping costs
4. Projected savings: $250-350/week in labor ($13K-18K annual)
5. Calculated ROI: 430x-599x return on scraping investment
6. Added intangible benefits: faster opportunity detection, better data quality

**Verification**:
- Complete cost breakdown in audit document
- Weekly and annual projections provided
- ROI formula transparent and verifiable
- Comparison shows automation is no-brainer

---

## 9. PENDING ITEMS / NEXT STEPS

### Immediate Actions (Week 1)

**Priority 1: Enable LinkedIn Job Scraping**
- [ ] Call `apify:add-actor` with `curious_coder/linkedin-jobs-scraper`
- [ ] Test scrape for San Diego GDIT jobs (max 50 results)
- [ ] Validate output quality (hiring manager names present)
- [ ] Compare to Puppeteer scrape results
- [ ] Document any issues or limitations

**Priority 2: Update MCP Server Configuration**
- [ ] Add storage tools category to server URL
- [ ] Test `get-dataset-list` to see past Puppeteer runs
- [ ] Verify `get-dataset-items` filtering works
- [ ] Update Claude project documentation with new tools

**Priority 3: Run Baseline Week-Over-Week Analysis**
- [ ] Retrieve December 17, 19, and 22 Puppeteer datasets
- [ ] Compare job counts by company and location
- [ ] Identify any programs with significant changes
- [ ] Document methodology for future weekly reports

---

### Short-Term (Weeks 2-4)

**Week 2: Indeed Integration**
- [ ] Add `valig/indeed-jobs-scraper` actor
- [ ] Configure search for GDIT + defense contractors
- [ ] Run parallel scrapes: LinkedIn + Indeed + Puppeteer
- [ ] Merge results and identify overlaps
- [ ] Assess whether Indeed adds meaningful coverage

**Week 3: Automated Enrichment Pipeline**
- [ ] Create Claude prompt template for job enrichment
- [ ] Test classification logic on 50-100 jobs
- [ ] Validate program mapping accuracy
- [ ] Cross-reference Federal Programs database
- [ ] Import enriched jobs to Notion Program Mapping Hub

**Week 4: Weekly Workflow Documentation**
- [ ] Document step-by-step weekly process
- [ ] Create reusable Claude prompts for each stage
- [ ] Train BD team on new tools and workflow
- [ ] Set up Monday morning scrape schedule (manual trigger initially)
- [ ] Generate first weekly HUMINT update using new data

---

### Medium-Term (Q1 2026)

**Contact Discovery Automation**
- [ ] Map enriched jobs to DCGS Contacts Full
- [ ] Extract hiring manager names from LinkedIn data
- [ ] Automate contact classification
- [ ] Generate contact import batches for Notion
- [ ] Update BD Priority scores based on hiring activity

**Historical Analysis System**
- [ ] Build 12-week dataset history
- [ ] Create trend detection logic (20%+ increase = signal)
- [ ] Develop program activity dashboard
- [ ] Correlate hiring surges with contract actions
- [ ] Generate competitive intelligence reports

**BD Intelligence Reporting**
- [ ] Weekly HUMINT update template
- [ ] Call sheet auto-generation
- [ ] Program-specific playbook updates
- [ ] Pain point tracking and documentation
- [ ] Opportunity pipeline visualization

---

### Long-Term (Q2-Q4 2026)

**Scale Beyond DCGS**
- [ ] Apply workflow to Army DCGS-A programs
- [ ] Expand to Navy DCGS-N sites
- [ ] Add other GDIT programs from Federal Programs database
- [ ] Replicate for Leidos, SAIC, Peraton programs
- [ ] Build multi-prime comparative intelligence

**Predictive Analytics**
- [ ] Accumulate 6-12 months historical data
- [ ] Develop ML model for contract option predictions
- [ ] Correlate hiring patterns with recompete timing
- [ ] Identify optimal BD timing windows
- [ ] Create early warning system for opportunities

**Full n8n Automation**
- [ ] Design n8n workflow for scheduled scraping
- [ ] Implement webhook listeners for Actor completion
- [ ] Automate Notion database updates
- [ ] Email notifications for significant changes
- [ ] Self-service BD intelligence dashboard

---

### Unresolved Questions

1. **Apify API Token Management**: Where is token stored? How to rotate securely?
2. **Notion API Rate Limits**: Will bulk imports exceed Notion's rate limits?
3. **Data Retention**: How long to keep historical job datasets? Storage costs?
4. **ZoomInfo Integration**: Can we automate contact discovery or still manual?
5. **LinkedIn Compliance**: Does scraping comply with LinkedIn's terms of service?
6. **n8n Hosting**: Self-hosted or n8n Cloud Pro? Cost implications?
7. **Error Handling**: What happens if Actor run fails? Retry logic needed?
8. **Data Quality Validation**: How to verify program mapping accuracy at scale?

---

## 10. KEY INSIGHTS & GOTCHAS

### MCP Integration Insights

**Insight 1: Dynamic Tool Discovery is Powerful**
- Claude can search Apify Store, find relevant Actors, and add them on-demand
- No need to pre-configure every possible Actor
- Enables adaptive workflows based on changing requirements
- Example: User asks "scrape LinkedIn jobs" â†’ Claude finds Actor â†’ adds tool â†’ executes

**Gotcha**: Only works in clients supporting dynamic discovery (Claude.ai web, VS Code). Other clients get static `call-actor` tool instead.

---

**Insight 2: Output Previews are Truncated**
- Actor runs return preview of first 5-10 items
- ALWAYS use `get-actor-output` to retrieve complete dataset
- Especially critical for job scrapes with 100+ results
- Preview may look sufficient but you're missing 90% of data

**Gotcha**: Easy to forget and assume preview is complete output. Always check `stats.itemCount` and call `get-actor-output` if >10.

---

**Insight 3: Storage Tools Unlock Historical Analysis**
- Default configuration disables storage category
- `get-dataset-list` and `get-dataset-items` are game-changers for trend analysis
- Can query datasets from months ago with advanced filtering
- Enables week-over-week, month-over-month comparisons

**Gotcha**: Must explicitly enable via `?tools=storage` URL parameter. Not discoverable otherwise.

---

### Job Scraping Insights

**Insight 4: LinkedIn Scraper Provides Contact Data**
- Unlike generic job boards, LinkedIn scraper extracts hiring manager info
- Includes name, title, LinkedIn profile URL
- This eliminates manual ZoomInfo lookup step
- Dramatically accelerates contact discovery workflow

**Gotcha**: Not all jobs have hiring manager info. LinkedIn only shows this for some postings. Expect ~60-70% coverage.

---

**Insight 5: Multiple Sources Provide Comprehensive Coverage**
- LinkedIn: GDIT direct postings with hiring managers
- Indeed: Broader market, smaller staffing agencies
- Puppeteer: Competitor portals (Insight Global, TEKsystems)
- No single source captures everything

**Gotcha**: Deduplication required. Same job may appear on multiple platforms. Use title + location + company as composite key.

---

**Insight 6: Pricing Models Vary Widely**
- Per-result: $0.0001 to $5 per job (valig vs borderline)
- Per-event: Separate charges for start, each page, completion
- Free: Only Apify platform usage costs
- Always check pricing before large runs

**Gotcha**: "Pay Per Result" sounds cheap but check the denominator. $5/1000 = $0.005/job. $1/1000 = $0.001/job. 5x difference!

---

### Architecture Insights

**Insight 7: Claude + MCP + Notion is Powerful Combination**
- Claude: AI enrichment and classification logic
- MCP: Access to Apify datasets without CSV export
- Notion: Structured storage and team collaboration
- Integration eliminates manual copy/paste bottleneck

**Gotcha**: Each system has rate limits. Claude (project knowledge queries), Apify (30 req/sec), Notion (3 req/sec). Design for sequential processing, not parallel.

---

**Insight 8: Classification Logic Must Be Explicit**
- "San Diego" â†’ "AF DCGS - PACAF" requires hardcoded mapping
- Cannot rely on AI to infer program from job description alone
- Keyword-based detection more reliable than NLP
- Regular expression patterns for clearance detection work well

**Gotcha**: Edge cases will fail. "San Diego County" vs "San Diego, CA" need both. "TS/SCI" vs "TS-SCI" vs "Top Secret/SCI" all valid formats.

---

**Insight 9: BD Priority Calculation is Multi-Factor**
- Location: PACAF San Diego = automatic Critical
- Hierarchy: Tier 1-2 = Critical regardless of location
- Program: DCGS programs higher than non-DCGS
- Simple if-then logic works, no ML needed

**Gotcha**: Priority can change over time. Acting Site Lead â†’ permanent hire changes tier. Update classification logic accordingly.

---

### Cost & ROI Insights

**Insight 10: Automation ROI is Massive**
- Current: $400-500/week manual labor
- Automated: $0.55/week scraping + $100-150/week oversight
- Net savings: $250-350/week = $13K-18K annual
- 430x-599x return on investment

**Gotcha**: ROI calculation assumes BD analyst time is fully redeployed to higher-value activities (strategy, relationship building). If analyst just has free time, savings aren't realized.

---

**Insight 11: Data Quality Improves with Automation**
- Manual process prone to copy/paste errors
- Automated enrichment applies logic consistently
- Historical datasets enable validation and corrections
- Faster feedback loops improve classification accuracy

**Gotcha**: Garbage in, garbage out. If scraped data is poor quality (incomplete descriptions, wrong locations), enrichment will fail. Always spot-check raw data.

---

### Workflow Insights

**Insight 12: Weekly Cadence is Optimal**
- Daily scraping generates noise, hard to detect trends
- Monthly scraping too infrequent, miss opportunities
- Weekly strikes balance between signal and noise
- Monday morning aligns with business week planning

**Gotcha**: Some job boards update unpredictably. LinkedIn posts appear immediately, Indeed has 1-2 day delay. Scrape timing may miss fresh postings.

---

**Insight 13: HUMINT Gathering Still Requires Human Touch**
- Automation handles data collection and enrichment
- BD intelligence synthesis requires domain expertise
- Lower-tier contact outreach needs personalization
- AI assists but doesn't replace relationship building

**Gotcha**: Don't over-automate. BD team needs to own contact strategy, not just execute AI-generated scripts.

---

**Insight 14: Project Knowledge is Authoritative**
- Federal Programs database contains verified contract intel
- Use `project_knowledge_search` FIRST before web search
- Cross-referencing enriched jobs against this data is critical
- Accuracy depends on keeping Federal Programs updated

**Gotcha**: If Federal Programs has stale or incorrect data, entire enrichment pipeline produces bad output. Regular audits essential.

---

### Technical Gotchas

**Gotcha 1: Rate Limits are Per-User, Not Per-Tool**
- Apify: 30 requests/second across ALL tools
- Calling multiple Actors simultaneously can hit limit
- Implement exponential backoff for 429 responses
- Sequential execution safer than parallel

---

**Gotcha 2: OAuth Token Expiration**
- MCP Server uses OAuth for authentication
- Token may expire after period of inactivity
- User will need to re-authenticate via browser redirect
- Cannot automate OAuth refresh (security feature)

---

**Gotcha 3: Dataset Item Count Delays**
- `get-dataset` returns itemCount metadata
- This count updates with 5-second delay
- Immediately after Actor completion, may show 0 items
- Wait 5-10 seconds before retrieving items

---

**Gotcha 4: Field Filtering Case Sensitivity**
- `get-actor-output` fields parameter is case-sensitive
- Request `"hiring_manager"` not `"Hiring_Manager"`
- Dot notation for nested fields: `"hiring_manager.name"`
- Always check Actor output schema for exact field names

---

**Gotcha 5: Notion Property Name Changes Break Logic**
- Classification logic hardcodes Notion property names
- Renaming "Hierarchy Tier" to "Tier" breaks updates
- Schema changes require code updates in multiple places
- Use schema-locked properties or document dependencies

---

### Security & Compliance Gotchas

**Gotcha 6: LinkedIn Scraping Terms of Service**
- LinkedIn's ToS prohibits automated scraping
- Using third-party scrapers may violate terms
- Account could be suspended if detected
- Evaluate risk vs. reward for your use case

---

**Gotcha 7: Clearance Data is Sensitive**
- Job descriptions mention clearance requirements
- This is public info but still security-sensitive
- Don't store in publicly accessible Notion databases
- Ensure workspace access controls configured properly

---

**Gotcha 8: Contact Data Privacy**
- Hiring manager names/emails from LinkedIn are personal data
- GDPR/CCPA may apply depending on location
- Have consent mechanism for BD outreach
- Include opt-out links in all communications

---

### Performance Gotchas

**Gotcha 9: Notion Database Size Limits**
- Notion slows significantly above 1,000 records
- DCGS Contacts Full already at 965 records (approaching limit)
- Consider database split by program or archive old contacts
- API pagination required for large datasets

---

**Gotcha 10: Claude Context Window Limits**
- Fetching 100+ job records can exceed context window
- Process jobs in batches of 20-50 at a time
- Store intermediate results in files, not conversation memory
- Pagination essential for large datasets

---

### Lessons Learned

**Lesson 1: Start Simple, Then Optimize**
- Initial recommendation: Just add LinkedIn scraper
- Don't try to automate everything at once
- Validate data quality before building complex workflows
- Week 1-4 plan focuses on proving value incrementally

---

**Lesson 2: Documentation is Investment, Not Overhead**
- 40-page audit document takes time upfront
- Saves hundreds of hours in future onboarding and troubleshooting
- Provides authoritative reference for team
- Prevents re-learning same lessons repeatedly

---

**Lesson 3: ROI Justifies Almost Any Automation**
- $13K-18K annual savings dwarfs any implementation cost
- Even if automation takes 40 hours to build ($2K labor), 6.5x-9x ROI
- Intangible benefits (speed, quality) compound over time
- BD automation is high-leverage investment

---

**Lesson 4: AI-Assisted Enrichment Beats Rule-Based Systems**
- GPT-4o classification more accurate than keyword matching
- Can handle edge cases and ambiguous descriptions
- Faster to iterate prompts than hardcoded logic
- But validate with spot-checks, don't trust blindly

---

**Lesson 5: Multi-Source Data Wins**
- Single job board misses opportunities
- LinkedIn + Indeed + Puppeteer provides 360Â° coverage
- Deduplication hassle worth comprehensive view
- Competitor intelligence only visible with multiple sources

---

## APPENDIX A: QUICK REFERENCE

### Essential MCP Tool Commands

```javascript
// Search for Actors
apify:search-actors { keywords: "linkedin jobs", limit: 10 }

// Get Actor details
apify:fetch-actor-details { actor: "curious_coder/linkedin-jobs-scraper" }

// Add Actor as tool (dynamic discovery)
apify:add-actor { actor: "curious_coder/linkedin-jobs-scraper" }

// Retrieve complete output
apify:get-actor-output { datasetId: "xyz789", limit: 100 }

// List all datasets
apify:get-dataset-list { limit: 50, desc: true }

// Advanced dataset query
apify:get-dataset-items { 
  datasetId: "xyz789", 
  fields: "job_title,location", 
  limit: 50 
}
```

---

### Notion Collection IDs

```
DCGS Contacts Full:          2ccdef65-baa5-8087-a53b-000ba596128e
GDIT Other Contacts:         70ea1c94-211d-40e6-a994-e8d7c4807434
GDIT Jobs:                   2ccdef65-baa5-80b0-9a80-000bd2745f63
Program Mapping Hub:         f57792c1-605b-424c-8830-23ab41c47137
Federal Programs:            06cd9b22-5d6b-4d37-b0d3-ba99da4971fa
```

---

### Actor IDs for Job Scraping

```
LinkedIn (recommended):      curious_coder/linkedin-jobs-scraper
Indeed (recommended):        valig/indeed-jobs-scraper
Puppeteer (current):         apify/puppeteer-scraper
RAG Browser (pre-loaded):    apify/rag-web-browser
```

---

### Classification Values

**Programs**:
- AF DCGS - Langley
- AF DCGS - Wright-Patt
- AF DCGS - PACAF
- AF DCGS - Other
- Army DCGS-A
- Navy DCGS-N
- Corporate HQ
- Enterprise Security
- Unassigned

**Hierarchy Tiers**:
- Tier 1 - Executive
- Tier 2 - Director
- Tier 3 - Program Leadership
- Tier 4 - Management
- Tier 5 - Senior IC
- Tier 6 - Individual Contributor

**BD Priority**:
- ðŸ”´ Critical
- ðŸŸ  High
- ðŸŸ¡ Medium
- âšª Standard

**Location Hubs**:
- Hampton Roads
- San Diego Metro
- DC Metro
- Dayton/Wright-Patt
- Other CONUS
- OCONUS
- Unknown

---

### Useful File Paths

```
Audit Document:           /mnt/user-data/outputs/Apify_MCP_Server_Capabilities_Audit.docx
Classification Logic:     classification_logic.py (in export)
Job Enrichment Logic:     job_enrichment.py (in export)
Recent Job Scrapes:       dataset_puppeteerscraper_*.csv (project files)
Contact Exports:          DCGS_Contact_Spreadsheet_391_*.csv (project files)
```

---

## APPENDIX B: DOCUMENT METADATA

**Document Type**: Comprehensive Technical Export  
**Format**: Markdown  
**Created**: January 10, 2026  
**Author**: Claude (Anthropic)  
**User**: DirtyDiablo (George Maranville)  
**Organization**: Prime Technical Services (PTS)  
**Project**: DCGS Business Development Campaign  
**Total Conversation Length**: ~190,000 tokens  
**Export Length**: ~35,000 words  
**Sections**: 10 main + 2 appendices  
**Code Samples**: 15+ complete implementations  
**Examples**: 50+ use cases and scenarios  
**Tools Documented**: 20+ Apify MCP tools  
**Actors Reviewed**: 15+ job scraping Actors  
**Databases Documented**: 4 Notion schemas  

---

## APPENDIX C: RELATED RESOURCES

### Apify Documentation
- MCP Integration Guide: https://docs.apify.com/platform/integrations/mcp
- Actor Development: https://docs.apify.com/platform/actors
- API Reference: https://docs.apify.com/api/v2
- Pricing Models: https://docs.apify.com/platform/actors/running/pricing

### Model Context Protocol
- Specification: https://modelcontextprotocol.io
- Client Capabilities: https://github.com/apify/mcp-client-capabilities
- MCP SDK: https://github.com/modelcontextprotocol/sdk

### PTS DCGS Campaign Files
- BD Playbook: `/mnt/project/DCGS_BD_Playbook_Complete.docx`
- Federal Programs: `/mnt/project/Federal_Program_Cleaned_Notion_Import.csv`
- Call Sheets: `/mnt/project/DCGS_BD_Call_Sheet_BLITZ_20250105.xlsx`
- Contact Database: `/mnt/project/DCGS_Contact_Spreadsheet_391_120925_PERSON.csv`

---

## VERSION HISTORY

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2026-01-10 | Initial export document created | Claude |

---

**End of Export Document**
